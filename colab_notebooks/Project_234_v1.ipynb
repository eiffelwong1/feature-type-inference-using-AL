{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python [conda env:234proj]",
      "language": "python",
      "name": "conda-env-234proj-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "Project_234_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eiffelwong1/feature-type-inference-using-AL/blob/main/colab_notebooks/Project_234_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4yzwH6hW0d7",
        "outputId": "742a1fce-ce65-4329-9744-3b9364d252c2"
      },
      "source": [
        "!pip install modAL"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting modAL\n",
            "  Downloading modAL-0.4.1-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from modAL) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->modAL) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->modAL) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->modAL) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->modAL) (1.0.1)\n",
            "Installing collected packages: modAL\n",
            "Successfully installed modAL-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aubVrU9VVqoE"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from modAL.models import ActiveLearner\n",
        "from modAL.batch import uncertainty_batch_sampling\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVVSF_H5VqoF",
        "outputId": "bb5c4530-a0ef-4aa8-e20e-230762514877"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "xtrain = pd.read_csv('/content/drive/My Drive/Project_234/data_train.csv')\n",
        "xtest = pd.read_csv('/content/drive/My Drive/Project_234/data_test.csv')\n",
        "\n",
        "print(xtrain.shape)\n",
        "print(xtest.shape)\n",
        "print(xtrain.columns)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "(7936, 33)\n",
            "(1985, 33)\n",
            "Index(['Record_id', 'Attribute_name', 'y_act', 'total_vals', 'num_nans',\n",
            "       '%_nans', 'num_of_dist_val', '%_dist_val', 'mean', 'std_dev', 'min_val',\n",
            "       'max_val', 'sample_1', 'sample_2', 'sample_3', 'sample_4', 'sample_5',\n",
            "       'has_delimiters', 'has_url', 'has_email', 'has_date', 'mean_word_count',\n",
            "       'std_dev_word_count', 'mean_stopword_total', 'stdev_stopword_total',\n",
            "       'mean_char_count', 'stdev_char_count', 'mean_whitespace_count',\n",
            "       'stdev_whitespace_count', 'mean_delim_count', 'stdev_delim_count',\n",
            "       'is_list', 'is_long_sentence'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7SFzayyVqoG"
      },
      "source": [
        "filePathName = '/content/drive/My Drive/Project_234/RandomForest.pkl'\n",
        "loaded_model = pickle.load(open(filePathName, 'rb'))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfaMFCF-VqoG"
      },
      "source": [
        "useStats = 1\n",
        "useAttributeName = 1\n",
        "useSample1 = 0\n",
        "useSample2 = 0\n",
        "## Using descriptive stats and attribute name"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VovEeB7zVqoH"
      },
      "source": [
        "dict_label = {\n",
        "    'numeric': 0,\n",
        "    'categorical': 1,\n",
        "    'datetime': 2,\n",
        "    'sentence': 3,\n",
        "    'url': 4,\n",
        "    'embedded-number': 5,\n",
        "    'list': 6,\n",
        "    'not-generalizable': 7,\n",
        "    'context-specific': 8\n",
        "}\n",
        "\n",
        "y_train = xtrain.loc[:,['y_act']]\n",
        "y_test = xtest.loc[:,['y_act']]\n",
        "y_train['y_act'] = [dict_label[i] for i in y_train['y_act']]\n",
        "y_test['y_act'] = [dict_label[i] for i in y_test['y_act']]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gN2HUGcVqoH",
        "outputId": "cdee8b11-30e7-416a-f08d-3bdb80ff8bfb"
      },
      "source": [
        "def ProcessStats(data,y):\n",
        "\n",
        "    data1 = data[['total_vals', 'num_nans', '%_nans', 'num_of_dist_val', '%_dist_val', 'mean', 'std_dev', 'min_val', 'max_val','has_delimiters', 'has_url', 'has_email', 'has_date', 'mean_word_count',\n",
        "       'std_dev_word_count', 'mean_stopword_total', 'stdev_stopword_total',\n",
        "       'mean_char_count', 'stdev_char_count', 'mean_whitespace_count',\n",
        "       'stdev_whitespace_count', 'mean_delim_count', 'stdev_delim_count',\n",
        "       'is_list', 'is_long_sentence']]\n",
        "    data1 = data1.reset_index(drop=True)\n",
        "    data1 = data1.fillna(0)\n",
        "\n",
        "    y.y_act = y.y_act.astype(float)\n",
        "    \n",
        "    return data1\n",
        "\n",
        "vectorizerName = CountVectorizer(ngram_range=(2, 2), analyzer='char')\n",
        "vectorizerSample = CountVectorizer(ngram_range=(2, 2), analyzer='char')\n",
        "\n",
        "def FeatureExtraction(data,data1,flag):\n",
        "\n",
        "    arr = data['Attribute_name'].values\n",
        "    arr = [str(x) for x in arr]\n",
        "    \n",
        "    arr1 = data['sample_1'].values\n",
        "    arr1 = [str(x) for x in arr1]\n",
        "    arr2 = data['sample_2'].values\n",
        "    arr2 = [str(x) for x in arr2]\n",
        "    arr3 = data['sample_3'].values\n",
        "    arr3 = [str(x) for x in arr3]    \n",
        "    print(len(arr1),len(arr2))\n",
        "    if flag:\n",
        "        X = vectorizerName.fit_transform(arr)\n",
        "        X1 = vectorizerSample.fit_transform(arr1)\n",
        "        X2 = vectorizerSample.transform(arr2)   \n",
        "    else:\n",
        "        X = vectorizerName.transform(arr)\n",
        "        X1 = vectorizerSample.transform(arr1)\n",
        "        X2 = vectorizerSample.transform(arr2)        \n",
        "        \n",
        "#     print(f\"> Length of vectorized feature_names: {len(vectorizer.get_feature_names())}\")\n",
        "\n",
        "    attr_df = pd.DataFrame(X.toarray())\n",
        "    sample1_df = pd.DataFrame(X1.toarray())\n",
        "    sample2_df = pd.DataFrame(X2.toarray())\n",
        "    print(len(data1),len(attr_df),len(sample1_df),len(sample2_df))\n",
        "\n",
        "    if useSample1: data2 = sample1_df\n",
        "    if useSample2: data2 = sample2_df    \n",
        "    \n",
        "    data2 = pd.concat([data1, attr_df], axis=1, sort=False)\n",
        "    print(len(data2))\n",
        "    return data2\n",
        "\n",
        "xtrain1 = ProcessStats(xtrain,y_train)\n",
        "xtest1 = ProcessStats(xtest,y_test)\n",
        "\n",
        "\n",
        "X_train = FeatureExtraction(xtrain,xtrain1,1)\n",
        "X_test = FeatureExtraction(xtest,xtest1,0)\n",
        "\n",
        "\n",
        "X_train_new = X_train.reset_index(drop=True)\n",
        "y_train_new = y_train.reset_index(drop=True)\n",
        "X_train_new = X_train_new.values\n",
        "y_train_new = y_train_new.values"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7936 7936\n",
            "7936 7936 7936 7936\n",
            "7936\n",
            "1985 1985\n",
            "1985 1985 1985 1985\n",
            "1985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ORxyCh0VqoI",
        "outputId": "ba9bde11-76fd-4ecf-e232-2a06acbdf8e8"
      },
      "source": [
        "print(loaded_model.predict(X_train_new))\n",
        "\n",
        "print(\"train acc. :\",loaded_model.score(X_train_new, y_train))\n",
        "print(\"test  acc. :\",loaded_model.score(X_test, y_test))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. ... 3. 3. 3.]\n",
            "train acc. : 0.9731602822580645\n",
            "test  acc. : 0.926448362720403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-QHTlpgVqoJ",
        "outputId": "9b68905b-7f80-4b51-f7a4-ea4658234e1b"
      },
      "source": [
        "print([i for i in X_train_new[0]])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[21477, 0, 0.0, 174, 0.810169018, 0.0, 0.0, 0.0, 0.0, False, False, False, False, 1.4, 0.8, 0.2, 0.4, 10.0, 4.816637832, 0.4, 0.8, 0.4, 0.8, False, False, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fo3dB9NMbaLn",
        "outputId": "75b312d7-e2cf-4dfd-ffb6-f31035777eed"
      },
      "source": [
        "print(len(X_train_new))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VjodBfxVqoJ"
      },
      "source": [
        "# keep_percent = 0.2\n",
        "query_sizes = [25] #[5, 10, 25, 50, 100]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6_4IDabKiGA8",
        "outputId": "f15895bb-c3c5-459d-d175-ea85910f2318"
      },
      "source": [
        "with warnings.catch_warnings(record=\"True\"):\n",
        "  # ##### Version 1: Split test set into simulation set and actual test set\n",
        "  # simulation_idx = np.random.choice(len(X_test), int(len(X_test)/2), replace=False)\n",
        "  # test_idx = [i for i in range(len(X_test)) if i not in simulation_idx]\n",
        "  # X_simulation_AL = []\n",
        "  # y_simulation_AL = []\n",
        "  # for idx in simulation_idx:\n",
        "  #   X_simulation_AL.append(X_test.iloc[idx]) \n",
        "  #   y_simulation_AL.append(y_test.iloc[idx])\n",
        "\n",
        "  # X_test_AL = []\n",
        "  # y_test_AL = []\n",
        "  # for idx in test_idx:\n",
        "  #   X_test_AL.append(X_test.iloc[idx]) \n",
        "  #   y_test_AL.append(y_test.iloc[idx])\n",
        "\n",
        "  ##### Version 2: Reduce train set from 7936 to 6000 - use the remaining 1936 samples for AL and use the test set as is (1936/7936 ~= 24.39 %)\n",
        "  X_train_AL, X_simulation_AL, y_train_AL, y_simulation_AL = train_test_split(X_train_new, y_train, test_size=0.35, random_state=4, stratify=y_train, shuffle=True)\n",
        "\n",
        "  # Start AL loops simulated with a part of the test data\n",
        "  X_simulation_df = pd.DataFrame(X_simulation_AL)\n",
        "  y_simulation_df = pd.DataFrame(y_simulation_AL)\n",
        "\n",
        "  # making copies of existing X_test and y_test data\n",
        "  X_test_AL = pd.DataFrame(X_test)\n",
        "  y_test_AL = pd.DataFrame(y_test)\n",
        "\n",
        "  print(f\"Training set size: {len(X_train_AL)}, Simulation set size: {len(X_simulation_df)}, Test set size: {len(X_test_AL)}\")\n",
        "\n",
        "  # X_simulation_df = X_simulation_df.reset_index(drop=True)\n",
        "  for query_size in query_sizes:\n",
        "    model_accuracies = []\n",
        "    feed_to_learner = []\n",
        "\n",
        "    preset_batch = partial(uncertainty_batch_sampling, n_instances=query_size)\n",
        "    learners = []\n",
        "    n_estimators_grid = [5, 25, 50, 75, 100, 500]\n",
        "    max_depth_grid = [5, 10, 25, 50, 100, 250]\n",
        "    best_model_score = 0\n",
        "    best_model_idx = 0\n",
        "\n",
        "    # picking the best model after doing hyperparam tuning (normal grid search)\n",
        "    for ne in n_estimators_grid:\n",
        "        for md in max_depth_grid:\n",
        "            learner = ActiveLearner(\n",
        "                  estimator=RandomForestClassifier(n_estimators=ne, max_depth=md, random_state=100),\n",
        "                  X_training=X_train_AL, y_training=y_train_AL,\n",
        "                  query_strategy=preset_batch\n",
        "                )\n",
        "            score = learner.score(X_test_AL, y_test_AL)\n",
        "            print(f\"[n_estimator: {ne}, max_depth: {md}, accuracy: {score}]\")\n",
        "            if best_model_score < score:\n",
        "                best_ne = ne\n",
        "                best_md = md\n",
        "                best_model_score = score\n",
        "                bestPerformingModel = learner\n",
        "\n",
        "    print(f\"[BEST OBTAINED RF ESTIMATOR] === [n_estimator: {best_ne}, max_depth: {best_md}, accuracy: {best_model_score}]\")\n",
        "    num_of_queries = int(len(X_simulation_df)/query_size)\n",
        "    print(f\"\\n>>>>>>>>>>>>>> [NEW ITERATION WITH NEXT QUERY SIZE] The total number of queries: {num_of_queries} <<<<<<<<<<<<<<\\n\")\n",
        "    \n",
        "    unqueried_score = learner.score(X_test_AL, y_test_AL)\n",
        "    unqueried_score_train = learner.score(X_train_AL, y_train_AL)\n",
        "    model_accuracies.append(unqueried_score)\n",
        "    print(f\"[INITIAL] Unqueried score before the AL teaching on TRAIN set: {unqueried_score_train}\")\n",
        "    print(f\"[INITIAL] Unqueried score before the AL teaching on TEST set: {unqueried_score}\")\n",
        "\n",
        "    for i in range(num_of_queries):\n",
        "      print(f\"\\n*********** QUERY {i+1} ***********\\n\")\n",
        "      print(f\"Length of simulation set in query {i+1} is: {len(X_simulation_df)}\")\n",
        "      query_idx, query_inst = learner.query(np.array(X_simulation_df))\n",
        "      print(f\"Nodes returned for query in iteration 1: {query_idx}\")\n",
        "      learner.teach(X = X_simulation_df.iloc[query_idx], y = y_simulation_df.iloc[query_idx])\n",
        "      feed_to_learner += list(query_idx)\n",
        "\n",
        "      # delete queries that have been looped back into the model\n",
        "      X_simulation_df = X_simulation_df.drop(X_simulation_df.index[query_idx])\n",
        "      y_simulation_df = y_simulation_df.drop(y_simulation_df.index[query_idx])\n",
        "\n",
        "      # learner.teach(X = pd.Series(X_simulation_np[idx]), y = y_simulation_np[idx])\n",
        "\n",
        "      # X_simulation_np = np.delete(X_simulation_np, query_index, axis=0)\n",
        "      # y_simulation_np = np.delete(y_simulation_np, query_index)\n",
        "\n",
        "      # Calculate and report our model's accuracy.\n",
        "      model_accuracy = learner.score(X_test_AL, y_test_AL)\n",
        "      model_accuracies.append(model_accuracy)\n",
        "      print(f\"\\n[INTERMEDIATE] Accuracy after query {i+1}: {model_accuracy:0.4f}\")\n",
        "\n",
        "  print(f\"\\n[FINAL] Accuracy after query {num_of_queries}: {model_accuracies[-1]:0.4f}\")\n",
        "  print(f\"\\n[FINAL-HIGHEST] OPTIMAL Accuracy obtained: {max(model_accuracies):0.4f}\")\n",
        "  print(f\"\\n[FINAL ACCURACY LIST]: {model_accuracies}\")\n",
        "  print(f\"\\n[FINAL FEED TO LEARNER LIST]: {len(feed_to_learner)}, {feed_to_learner}\")\n",
        "  plt.plot([1] + [i+1 for i in range(1, num_of_queries + 1)], model_accuracies)\n",
        "  plt.xlabel(\"Queries\")\n",
        "  plt.ylabel(\"Model accuracy on test set\")\n",
        "  plt.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 5158, Simulation set size: 2778, Test set size: 1985\n",
            "[n_estimator: 5, max_depth: 5, accuracy: 0.6453400503778337]\n",
            "[n_estimator: 5, max_depth: 10, accuracy: 0.7581863979848866]\n",
            "[n_estimator: 5, max_depth: 25, accuracy: 0.8493702770780857]\n",
            "[n_estimator: 5, max_depth: 50, accuracy: 0.8896725440806046]\n",
            "[n_estimator: 5, max_depth: 100, accuracy: 0.8906801007556675]\n",
            "[n_estimator: 5, max_depth: 250, accuracy: 0.8906801007556675]\n",
            "[n_estimator: 25, max_depth: 5, accuracy: 0.6790931989924434]\n",
            "[n_estimator: 25, max_depth: 10, accuracy: 0.7894206549118388]\n",
            "[n_estimator: 25, max_depth: 25, accuracy: 0.8952141057934508]\n",
            "[n_estimator: 25, max_depth: 50, accuracy: 0.9178841309823678]\n",
            "[n_estimator: 25, max_depth: 100, accuracy: 0.9158690176322418]\n",
            "[n_estimator: 25, max_depth: 250, accuracy: 0.9158690176322418]\n",
            "[n_estimator: 50, max_depth: 5, accuracy: 0.6750629722921915]\n",
            "[n_estimator: 50, max_depth: 10, accuracy: 0.7909319899244333]\n",
            "[n_estimator: 50, max_depth: 25, accuracy: 0.9017632241813602]\n",
            "[n_estimator: 50, max_depth: 50, accuracy: 0.9229219143576827]\n",
            "[n_estimator: 50, max_depth: 100, accuracy: 0.9178841309823678]\n",
            "[n_estimator: 50, max_depth: 250, accuracy: 0.9178841309823678]\n",
            "[n_estimator: 75, max_depth: 5, accuracy: 0.6755667506297229]\n",
            "[n_estimator: 75, max_depth: 10, accuracy: 0.8005037783375315]\n",
            "[n_estimator: 75, max_depth: 25, accuracy: 0.8992443324937027]\n",
            "[n_estimator: 75, max_depth: 50, accuracy: 0.9209068010075567]\n",
            "[n_estimator: 75, max_depth: 100, accuracy: 0.9214105793450882]\n",
            "[n_estimator: 75, max_depth: 250, accuracy: 0.9214105793450882]\n",
            "[n_estimator: 100, max_depth: 5, accuracy: 0.6780856423173803]\n",
            "[n_estimator: 100, max_depth: 10, accuracy: 0.7959697732997482]\n",
            "[n_estimator: 100, max_depth: 25, accuracy: 0.9037783375314862]\n",
            "[n_estimator: 100, max_depth: 50, accuracy: 0.9224181360201511]\n",
            "[n_estimator: 100, max_depth: 100, accuracy: 0.9224181360201511]\n",
            "[n_estimator: 100, max_depth: 250, accuracy: 0.9224181360201511]\n",
            "[n_estimator: 500, max_depth: 5, accuracy: 0.6952141057934509]\n",
            "[n_estimator: 500, max_depth: 10, accuracy: 0.7884130982367759]\n",
            "[n_estimator: 500, max_depth: 25, accuracy: 0.9002518891687658]\n",
            "[n_estimator: 500, max_depth: 50, accuracy: 0.9209068010075567]\n",
            "[n_estimator: 500, max_depth: 100, accuracy: 0.9259445843828715]\n",
            "[n_estimator: 500, max_depth: 250, accuracy: 0.9259445843828715]\n",
            "[BEST OBTAINED RF ESTIMATOR] === [n_estimator: 500, max_depth: 100, accuracy: 0.9259445843828715]\n",
            "\n",
            ">>>>>>>>>>>>>> [NEW ITERATION WITH NEXT QUERY SIZE] The total number of queries: 111 <<<<<<<<<<<<<<\n",
            "\n",
            "[INITIAL] Unqueried score before the AL teaching on TRAIN set: 0.9998061264055835\n",
            "[INITIAL] Unqueried score before the AL teaching on TEST set: 0.9259445843828715\n",
            "\n",
            "*********** QUERY 1 ***********\n",
            "\n",
            "Length of simulation set in query 1 is: 2778\n",
            "Nodes returned for query in iteration 1: [1136 1716   98  530  222 1685  241 1382 1516 1909  139  451  277   82\n",
            "  336 1942  323 1727 1323 1934 1537 2509 2262 1399 1510]\n",
            "\n",
            "[INTERMEDIATE] Accuracy after query 1: 0.9244\n",
            "\n",
            "*********** QUERY 2 ***********\n",
            "\n",
            "Length of simulation set in query 2 is: 2753\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-e6089e2ce8f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n*********** QUERY {i+1} ***********\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Length of simulation set in query {i+1} is: {len(X_simulation_df)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m       \u001b[0mquery_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_inst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_simulation_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Nodes returned for query in iteration 1: {query_idx}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_simulation_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_simulation_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/modAL/models/base.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, X_pool, *query_args, **query_kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mlabelled\u001b[0m \u001b[0mupon\u001b[0m \u001b[0mquery\u001b[0m \u001b[0msynthesis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mquery_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mquery_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mquery_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/modAL/batch.py\u001b[0m in \u001b[0;36muncertainty_batch_sampling\u001b[0;34m(classifier, X, n_instances, metric, n_jobs, **uncertainty_measure_kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0muncertainty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_uncertainty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0muncertainty_measure_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     return ranked_batch(classifier, unlabeled=X, uncertainty_scores=uncertainty,\n\u001b[0;32m--> 217\u001b[0;31m                                  n_instances=n_instances, metric=metric, n_jobs=n_jobs)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/modAL/batch.py\u001b[0m in \u001b[0;36mranked_batch\u001b[0;34m(classifier, unlabeled, uncertainty_scores, n_instances, metric, n_jobs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         instance_index, instance, mask = select_instance(X_training=labeled, X_pool=unlabeled,\n\u001b[1;32m    169\u001b[0m                                                          \u001b[0mX_uncertainty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muncertainty_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                                                          metric=metric, n_jobs=n_jobs)\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# Add our instance we've considered for labeling to our labeled set. Although we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/modAL/batch.py\u001b[0m in \u001b[0;36mselect_instance\u001b[0;34m(X_training, X_pool, X_uncertainty, mask, metric, n_jobs)\u001b[0m\n\u001b[1;32m     97\u001b[0m         _, distance_scores = pairwise_distances_argmin_min(X_pool_masked.reshape(n_unlabeled, -1),\n\u001b[1;32m     98\u001b[0m                                                            \u001b[0mX_training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_labeled_records\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                                                            metric=metric)\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         distance_scores = pairwise_distances(X_pool_masked.reshape(n_unlabeled, -1),\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_argmin_min\u001b[0;34m(X, Y, axis, metric, metric_kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m     indices, values = zip(*pairwise_distances_chunked(\n\u001b[1;32m    584\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_argmin_min_reduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         **metric_kwargs))\n\u001b[0m\u001b[1;32m    586\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   1593\u001b[0m             \u001b[0mX_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m         D_chunk = pairwise_distances(X_chunk, Y, metric=metric,\n\u001b[0;32m-> 1595\u001b[0;31m                                      n_jobs=n_jobs, **kwds)\n\u001b[0m\u001b[1;32m   1596\u001b[0m         if ((X is Y or Y is None)\n\u001b[1;32m   1597\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0mPAIRWISE_DISTANCE_FUNCTIONS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1752\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mYY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPGdpgmqrEIM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}