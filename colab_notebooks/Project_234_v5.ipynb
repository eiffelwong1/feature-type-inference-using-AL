{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4yzwH6hW0d7",
    "outputId": "46404182-729e-48e7-f189-8cc0afd5565a"
   },
   "outputs": [],
   "source": [
    "!pip install modAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aubVrU9VVqoE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import seaborn as sn\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import *\n",
    "from modAL.batch import uncertainty_batch_sampling\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "his_folder = \"./his\"\n",
    "img_folder = \"./img\"\n",
    "feed_folder = \"./feed\"\n",
    "log_folder = \"./log_new\"\n",
    "NP_SEED = 42\n",
    "\n",
    "np.random.seed(NP_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7SFzayyVqoG"
   },
   "outputs": [],
   "source": [
    "filePathName = './RandomForest.pkl'\n",
    "loaded_model = pickle.load(open(filePathName, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfaMFCF-VqoG"
   },
   "outputs": [],
   "source": [
    "useStats = 1\n",
    "useAttributeName = 1\n",
    "useSample1 = 0\n",
    "useSample2 = 0\n",
    "## Using descriptive stats and attribute name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VovEeB7zVqoH"
   },
   "outputs": [],
   "source": [
    "dict_label = {\n",
    "    'numeric': 0,\n",
    "    'categorical': 1,\n",
    "    'datetime': 2,\n",
    "    'sentence': 3,\n",
    "    'url': 4,\n",
    "    'embedded-number': 5,\n",
    "    'list': 6,\n",
    "    'not-generalizable': 7,\n",
    "    'context-specific': 8\n",
    "}\n",
    "\n",
    "dict_label_list = [k for k,v in sorted(dict_label.items(), key = lambda x: x[1])]\n",
    "print(dict_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3gN2HUGcVqoH",
    "outputId": "4d1b9a7c-e705-4afd-f17f-50fee930ca53"
   },
   "outputs": [],
   "source": [
    "def ProcessStats(data,y):\n",
    "    data1 = data[['total_vals', 'num_nans', '%_nans', 'num_of_dist_val', '%_dist_val', 'mean', 'std_dev', 'min_val', 'max_val','has_delimiters', 'has_url', 'has_email', 'has_date', 'mean_word_count',\n",
    "       'std_dev_word_count', 'mean_stopword_total', 'stdev_stopword_total',\n",
    "       'mean_char_count', 'stdev_char_count', 'mean_whitespace_count',\n",
    "       'stdev_whitespace_count', 'mean_delim_count', 'stdev_delim_count',\n",
    "       'is_list', 'is_long_sentence']]\n",
    "    data1 = data1.reset_index(drop=True)\n",
    "    data1 = data1.fillna(0)\n",
    "\n",
    "    y.y_act = y.y_act.astype(float)\n",
    "    \n",
    "    return data1\n",
    "\n",
    "vectorizerName = CountVectorizer(ngram_range=(2, 2), analyzer='char')\n",
    "vectorizerSample = CountVectorizer(ngram_range=(2, 2), analyzer='char')\n",
    "\n",
    "\n",
    "def FeatureExtraction(data,data1,flag):\n",
    "    arr = data['Attribute_name'].values\n",
    "    arr = [str(x) for x in arr]\n",
    "    \n",
    "    arr1 = data['sample_1'].values\n",
    "    arr1 = [str(x) for x in arr1]\n",
    "    arr2 = data['sample_2'].values\n",
    "    arr2 = [str(x) for x in arr2]\n",
    "    arr3 = data['sample_3'].values\n",
    "    arr3 = [str(x) for x in arr3]    \n",
    "    #print(len(arr1),len(arr2))\n",
    "    if flag:\n",
    "        X = vectorizerName.fit_transform(arr)\n",
    "        X1 = vectorizerSample.fit_transform(arr1)\n",
    "        X2 = vectorizerSample.transform(arr2)   \n",
    "    else:\n",
    "        X = vectorizerName.transform(arr)\n",
    "        X1 = vectorizerSample.transform(arr1)\n",
    "        X2 = vectorizerSample.transform(arr2)        \n",
    "        \n",
    "#     print(f\"> Length of vectorized feature_names: {len(vectorizer.get_feature_names())}\")\n",
    "\n",
    "    attr_df = pd.DataFrame(X.toarray())\n",
    "    sample1_df = pd.DataFrame(X1.toarray())\n",
    "    sample2_df = pd.DataFrame(X2.toarray())\n",
    "    #print(len(data1),len(attr_df),len(sample1_df),len(sample2_df))\n",
    "\n",
    "    if useSample1: data2 = sample1_df\n",
    "    if useSample2: data2 = sample2_df    \n",
    "    \n",
    "    data2 = pd.concat([data1, attr_df], axis=1, sort=False)\n",
    "    #print(len(data2))\n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sim_size = 0.8):\n",
    "    \"\"\"\n",
    "    sim_size is the % of training data that goes into simulation set.\n",
    "    \"\"\"\n",
    "    xtrain = pd.read_csv('./data_train.csv')\n",
    "    xtest = pd.read_csv('./data_test.csv')\n",
    "\n",
    "    y_train = xtrain.loc[:,['y_act']]\n",
    "    y_test = xtest.loc[:,['y_act']]\n",
    "    y_train['y_act'] = [dict_label[i] for i in y_train['y_act']]\n",
    "    y_test['y_act'] = [dict_label[i] for i in y_test['y_act']]\n",
    "    \n",
    "    xtrain1 = ProcessStats(xtrain,y_train)\n",
    "    xtest1 = ProcessStats(xtest,y_test)\n",
    "\n",
    "    X_train = FeatureExtraction(xtrain,xtrain1,1)\n",
    "    X_test = FeatureExtraction(xtest,xtest1,0)\n",
    "\n",
    "    X_train_new = X_train.reset_index(drop=True)\n",
    "    y_train_new = y_train.reset_index(drop=True)\n",
    "    X_train_new = X_train_new.values\n",
    "    y_train_new = y_train_new.values\n",
    "    \n",
    "    # setting up data\n",
    "    X_train_AL, X_simulation_AL, y_train_AL, y_simulation_AL = train_test_split(X_train_new, y_train, test_size=sim_size, random_state=NP_SEED, stratify = y_train)\n",
    "    # Start AL loops simulated with a part of the test data\n",
    "    X_simulation_df = pd.DataFrame(X_simulation_AL)\n",
    "    y_simulation_df = pd.DataFrame(y_simulation_AL)\n",
    "    # making copies of existing X_test and y_test data\n",
    "    X_test_df = pd.DataFrame(X_test)\n",
    "    y_test_df = pd.DataFrame(y_test)\n",
    "    \n",
    "    assert len(X_train_AL) == len(y_train_AL)\n",
    "    assert len(X_simulation_df) == len(y_simulation_df)\n",
    "    assert len(X_test_df) == len(y_test_df)\n",
    "    return X_train_AL, y_train_AL, X_simulation_df, y_simulation_df, X_test_df, y_test_df\n",
    "\n",
    "X_train, y_train, X_sim, y_sim, X_test, y_test = get_data()\n",
    "print(f\"Training set size: {len(X_train)}, Simulation set size: {len(X_sim)}, Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_sim.shape, y_sim.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def grid_parameter_search(X_train, y_train, X_test, y_test, n_estimators_grid, max_depth_grid, query_size):\n",
    "    best_model_score = 0\n",
    "    \n",
    "    preset_batch = partial(uncertainty_sampling, n_instances=query_size)\n",
    "    \n",
    "    for ne in n_estimators_grid:\n",
    "        for md in max_depth_grid:\n",
    "            learner = ActiveLearner(\n",
    "                  estimator=RandomForestClassifier(n_estimators=ne, max_depth=md, random_state=100),\n",
    "                  X_training=X_train, y_training=y_train,\n",
    "                  query_strategy=preset_batch\n",
    "                )\n",
    "            score = learner.score(X_test, y_test)\n",
    "            print(f\"[n_estimator: {ne}, max_depth: {md}, accuracy: {score}]\")\n",
    "            if best_model_score < score:\n",
    "                best_ne = ne\n",
    "                best_md = md\n",
    "                best_model_score = score\n",
    "                bestPerformingModel = learner\n",
    "    print(f\"Best model found at ne:{best_ne}, md:{best_md}, at score:{best_model_score}\")\n",
    "    return learner, best_ne, best_md, best_model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_his(data, full_path):\n",
    "#     assert data != None and len(data.items()) > 0, \"data should not be empty\"\n",
    "    \n",
    "#     with open(full_path, \"w\") as outfile:\n",
    "#         writer = csv.writer(outfile)\n",
    "#         writer.writerow(list(data.keys()))\n",
    "#         writer.writerows(zip(*data.values()))\n",
    "    \n",
    "\n",
    "# test_data_1 = [i for i in range(100)]\n",
    "# test_data_2 = [i*2 for i in range(100)]\n",
    "# test_data_3 = [i*3 for i in range(100)]\n",
    "# test_data_4 = [i*4 for i in range(100)]\n",
    "# data = {\"num_queries\": test_data_1, \"time\": test_data_2, \"total_time\":test_data_3, \"test_accuracy\":test_data_4}\n",
    "\n",
    "# save_his(data, his_folder + \"/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_matrix = np.random.rand(9,9)\n",
    "title = \"test confusion matrix\"\n",
    "\n",
    "def save_con_matrix(con_matrix, title, full_path, show=False):\n",
    "    plt.figure(figsize = (11,7))\n",
    "    df_cm = pd.DataFrame(con_matrix, index = dict_label_list, columns = dict_label_list)\n",
    "    sn.heatmap(con_matrix, annot=True, xticklabels=dict_label_list, yticklabels=dict_label_list, ) # font size\n",
    "\n",
    "    plt.title(title) # title with fontsize 20\n",
    "    plt.xlabel('predicted') # x-axis label with fontsize 15\n",
    "    plt.ylabel('true_value') # y-axis label with fontsize 15\n",
    "\n",
    "    plt.savefig(full_path)\n",
    "\n",
    "save_con_matrix(con_matrix, title, img_folder + \"/test_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feed = np.random.rand(9,9)\n",
    "\n",
    "def save_feed(feed, full_path):\n",
    "    file = open(full_path, 'wb')\n",
    "    pickle.dump(feed, file)\n",
    "    \n",
    "def load_feed(full_path):\n",
    "    file = open(full_path, 'rb')\n",
    "    return pickle.load(file)\n",
    "\n",
    "save_feed(test_feed, feed_folder + \"/test.pkl\")\n",
    "re = load_feed(feed_folder + \"/test.pkl\")\n",
    "\n",
    "assert np.all(test_feed == re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(file_name, *x):\n",
    "    line = ' '.join([str(a) for a in x])\n",
    "    open(file_name, \"a\").write(line + '\\n')\n",
    "    print(line)\n",
    "\n",
    "def confusion_mat_and_metrics(learner, X_test, y_test, matrix_folder, matrix_title, query_count):\n",
    "    # saving confusion matrix\n",
    "    y_pred = learner.predict(X_test)\n",
    "    con_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    file_path = f\"{matrix_folder}/queryCount_{query_count}\"\n",
    "    save_con_matrix(con_matrix, matrix_title + f\" - (query_count = {query_count})\", f\"{img_folder}/{file_path}.png\")\n",
    "    \n",
    "    return con_matrix.diagonal() / con_matrix.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPGdpgmqrEIM"
   },
   "outputs": [],
   "source": [
    "def train_this(n_est, max_dept, query_size, max_feed, per_iteration_acc_report = True, feed_his = None):\n",
    "    console_log_name = f\"./{log_folder}/train_log_ne{n_est}_md{max_dept}_qs{query_size}.log\"\n",
    "    metrics_log_name = f\"./{log_folder}/metrics_log_ne{n_est}_md{max_dept}_qs{query_size}.csv\"\n",
    "    console_log_file = open(console_log_name, \"a\")\n",
    "    metrics_log_file = open(metrics_log_name, \"a\")\n",
    "    \n",
    "    # getting data\n",
    "    X_train, y_train, X_sim, y_sim, X_test, y_test = get_data()\n",
    "    \n",
    "    # get learner\n",
    "    preset_batch = partial(uncertainty_batch_sampling, n_instances=query_size)\n",
    "    learner = ActiveLearner(\\\n",
    "    estimator=RandomForestClassifier(n_estimators=n_est, max_depth=max_dept, random_state=100),\n",
    "        X_training=X_train, y_training=y_train,\n",
    "        query_strategy=preset_batch\n",
    "        )\n",
    "    \n",
    "    # init tracker lists\n",
    "    num_queries = [0]\n",
    "    time_history = [0]\n",
    "    total_time_history = [0]\n",
    "    train_acc_history = [learner.score(X_train, y_train)]\n",
    "    sim_acc_history = [learner.score(X_sim, y_sim)]\n",
    "    test_acc_history = [learner.score(X_test, y_test)]\n",
    "    feed_to_learner = []\n",
    "    al_sampling = \"Uncertainty_Batch_Sampling\"\n",
    "    matrix_folder = f\"{al_sampling}_ne{n_est}_dept{max_dept}_querySize{query_size}\"\n",
    "    matrix_title = f\"AL using decision tree classifier (n_est = {n_est}, max_dept = {max_dept}) with {al_sampling} (query_size = {query_size})\"\n",
    "    Path(f\"{img_folder}/{matrix_folder}\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    log(console_log_name, f\"learner trained_acc:{train_acc_history}, sim_acc:{sim_acc_history}, test_acc:{test_acc_history}\")\n",
    "    with open(metrics_log_name, mode='w') as metrics_log_file:\n",
    "        metrics_writer = csv.writer(metrics_log_file, delimiter='|', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        metrics_writer.writerow(\"Query,Query_nodes,Simulation_accuracy,Test_accuracy,Train_accuracy,Query_time,Time_elapsed,Class_accuracies\".split(\",\"))\n",
    "    \n",
    "        # AL cycle\n",
    "        t_start = time.time()\n",
    "\n",
    "        X_train_df = pd.DataFrame(X_train)\n",
    "        y_train_df = pd.DataFrame(y_train)\n",
    "\n",
    "        while len(feed_to_learner) * query_size < max_feed :\n",
    "            current_count = len(feed_to_learner) * query_size\n",
    "            log(console_log_name, f\"\\n*********** QUERY {current_count} *********** remain:{len(X_sim)}\")\n",
    "            t0 = time.time()\n",
    "\n",
    "            #AL selection\n",
    "            query_idx = []\n",
    "\n",
    "            if feed_his:\n",
    "                query_idx = feed_his[0]\n",
    "                feed_his = feed_his[1:]\n",
    "            else:\n",
    "                query_idx, _ = learner.query(np.array(X_sim))\n",
    "                if current_count + len(query_idx) > max_feed:\n",
    "                    query_idx = query_idx[:current_count + len(query_idx) - max_feed]\n",
    "                    #query_inst = query_inst[:current_count + len(query_idx) - max_feed] \n",
    "\n",
    "            log(console_log_name, f\"Nodes returned for query in query {current_count}: {query_idx}\")\n",
    "\n",
    "            #updating learner\n",
    "            learner.teach(X = X_sim.iloc[query_idx], y = y_sim.iloc[query_idx])\n",
    "            feed_to_learner.append(list(query_idx))\n",
    "\n",
    "            # updating train set to calculate accuracy\n",
    "            X_train_df = X_train_df.append(X_sim.iloc[query_idx])\n",
    "            y_train_df = y_train_df.append(y_sim.iloc[query_idx])\n",
    "\n",
    "            # dropping queried entries from sim set\n",
    "            X_sim = X_sim.drop(X_sim.index[query_idx])\n",
    "            y_sim = y_sim.drop(y_sim.index[query_idx])\n",
    "\n",
    "            # recording history\n",
    "            t1 = time.time()\n",
    "            time_history.append(t1 - t0)\n",
    "            total_time_elapsed = t1 - t_start\n",
    "            num_queries.append(len(feed_to_learner)* query_size)\n",
    "\n",
    "            # Calculate and report our model's accuracy.\n",
    "            if per_iteration_acc_report:\n",
    "                test_acc = learner.score(X_test, y_test)\n",
    "                sim_acc = learner.score(X_sim, y_sim)\n",
    "                print(len(X_train_df), len(y_train_df))\n",
    "                train_acc = learner.score(X_train_df, y_train_df)\n",
    "\n",
    "                test_acc_history.append(  test_acc  )\n",
    "                sim_acc_history.append(   sim_acc   )\n",
    "                train_acc_history.append( train_acc )\n",
    "                log(console_log_name, f\"\\n[INTERMEDIATE] test_acc:{test_acc}, sim_acc:{sim_acc}, train_acc:{train_acc}\")\n",
    "\n",
    "            log(console_log_name, f\"\\n[INTERMEDIATE] Time taken for query {current_count}: {time_history[-1]}\")\n",
    "            log(console_log_name, f\"\\n[INTERMEDIATE] Total time taken {current_count}: {total_time_elapsed}\")\n",
    "\n",
    "            class_wise_acc = confusion_mat_and_metrics(learner, X_test, y_test, matrix_folder, matrix_title, current_count)\n",
    "            metrics_writer.writerow([current_count,query_idx,sim_acc,test_acc,train_acc,time_history[-1],total_time_elapsed,class_wise_acc])\n",
    "\n",
    "    # saving data into csv\n",
    "    log(console_log_name, f\"Saving data for for ne={n_est}, md={max_dept}, qs={query_size}\")\n",
    "    \n",
    "#     file_name = f\"{al_sampling}_ne{n_est}_dept{max_dept}_query{query_size}\"  \n",
    "#     data = {\"num_queries\": num_queries, \"time\": time_history, \"total_time\":total_time_history,\\\n",
    "#             \"train_accuracy\":train_acc_history, \\\n",
    "#             \"simulation_accuracy\":sim_acc_history, \\\n",
    "#             \"test_accuracy\":test_acc_history}\n",
    "#     save_his(data, f\"{his_folder}/{file_name}.csv\")\n",
    "    \n",
    "    # saving feed into pickle\n",
    "    save_feed(feed_to_learner, f\"{feed_folder}/{file_name}.pkl\")\n",
    "        \n",
    "    log(console_log_name, f\"Completed for ne={n_est}, md={max_dept}, qs={query_size}\")\n",
    "    metrics_log_file.close()\n",
    "    console_log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(record=\"True\"):       \n",
    "    for qs in [20]:\n",
    "        feed_his = None\n",
    "        learner, best_ne, best_md, best_model_score = grid_parameter_search(X_train, y_train, X_sim, y_sim, [i*100 for i in range(1, 5)],[i*100 for i in range(1, 5)], qs)\n",
    "        print(f\"[BEST OBTAINED RF ESTIMATOR] === [n_estimator: {best_ne}, max_depth: {best_md}, accuracy: {best_model_score}]\")\n",
    "        try:\n",
    "            file_name = f\"Uncertainty_Batch_Sampling_ne{best_ne}_dept{best_md}_query{qs}\"\n",
    "            feed_his = load_feed(f\"{feed_folder}/{file_name}.pkl\")\n",
    "        except:\n",
    "            print(f\"feed history for qs {qs} not found, starting fresh\")\n",
    "        train_this(n_est=best_ne, max_dept=best_md, query_size=qs, max_feed = 80, feed_his=feed_his)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(his_folder)\n",
    "\n",
    "def get_results():\n",
    "    for f in files:\n",
    "        if f.endswith(\".csv\") and not f.startswith(\"test\"):\n",
    "            with open(f\"{his_folder}/{f}\", mode='r') as infile:\n",
    "                reader = csv.reader(infile)\n",
    "                temp = zip(*reader)\n",
    "\n",
    "                results = {\"filename\": f}\n",
    "                for i in temp:\n",
    "                    results[i[0]] = i[1:]\n",
    "                yield results\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "for r in get_results():\n",
    "    print(r.keys())\n",
    "    plt.plot(r[\"num_queries\"], r[\"test_accuracy\"], label = r[\"filename\"])\n",
    "    \n",
    "plt.title(\"Accuracy of 1587 data training and additional 6340 data simluation\")\n",
    "plt.xlabel(\"# data fed by AL\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xticks(np.arange(0, 100, 10))\n",
    "plt.yticks(np.arange(0, 80, 10))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read log files and plot  - Need to plot after collection of data\n",
    "files = os.listdir(log_folder)\n",
    "query_time_dict = {}\n",
    "\n",
    "for f in files:\n",
    "    if f.startswith(\"metrics\"):\n",
    "        test_acc = []\n",
    "        train_acc = []\n",
    "        sim_acc = []\n",
    "        query_times = []\n",
    "        with open(f\"{log_folder}/{f}\", \"r\") as file:\n",
    "            for line in file:\n",
    "                if \"Simulation_accuracy\" in line:\n",
    "                    sim_acc.append(float(line.split(\"=\")[1]))\n",
    "                elif \"Train_accuracy\" in line:\n",
    "                    train_acc.append(float(line.split(\"=\")[1]))\n",
    "                elif \"Test_accuracy\" in line:\n",
    "                    test_acc.append(float(line.split(\"=\")[1]))\n",
    "                elif \"Time_elapsed\" in line:\n",
    "                    query_times.append(float(line.split(\"=\")[1]))\n",
    "        \n",
    "        x = [i for i in range(1, len(query_times) + 1)]\n",
    "        \n",
    "        fig, axs = plt.subplots(1, 3, figsize=(30,10))\n",
    "        axs[0].plot(x, test_acc)\n",
    "        axs[0].set_title('Test accuracies - ' + str(f))\n",
    "        axs[1].plot(x, train_acc, 'tab:orange', label=f)\n",
    "        axs[1].set_title('Train accuracies - ' + str(f))\n",
    "        axs[2].plot(x, sim_acc, 'tab:green', label=f)\n",
    "        axs[2].set_title('Simulation accuracies - ' + str(f))\n",
    "        query_time_dict[f] = query_times\n",
    "        \n",
    "#         print(f, len(sim_acc), len(train_acc), len(query_times), len(test_acc))\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='queries', ylabel='accuracy')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in query_time_dict.items():\n",
    "    plt.plot([i for i in range(1, len(v) + 1)], v, label = k)\n",
    "    print(f\"{k} ====> Total time = {round(sum(v), 2)}s & Average time per query = {round(sum(v) / len(v), 2)}s\")\n",
    "plt.xlabel(\"Time (secs)\")\n",
    "plt.ylabel(\"Queries\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project_234_v2.ipynb",
   "provenance": []
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
