{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4yzwH6hW0d7",
    "outputId": "46404182-729e-48e7-f189-8cc0afd5565a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: modAL in /home/siw011/.local/lib/python3.9/site-packages (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.13 in /opt/conda/lib/python3.9/site-packages (from modAL) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /opt/conda/lib/python3.9/site-packages (from modAL) (0.24.2)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from modAL) (1.3.4)\n",
      "Requirement already satisfied: scipy>=0.18 in /opt/conda/lib/python3.9/site-packages (from modAL) (1.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.1.0->modAL) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.1.0->modAL) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->modAL) (1.15.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.18->modAL) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.18->modAL) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install modAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aubVrU9VVqoE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import *\n",
    "from modAL.batch import uncertainty_batch_sampling\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "C7SFzayyVqoG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.22.2.post1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.22.2.post1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "filePathName = './RandomForest.pkl'\n",
    "loaded_model = pickle.load(open(filePathName, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YfaMFCF-VqoG"
   },
   "outputs": [],
   "source": [
    "useStats = 1\n",
    "useAttributeName = 1\n",
    "useSample1 = 0\n",
    "useSample2 = 0\n",
    "## Using descriptive stats and attribute name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VovEeB7zVqoH"
   },
   "outputs": [],
   "source": [
    "dict_label = {\n",
    "    'numeric': 0,\n",
    "    'categorical': 1,\n",
    "    'datetime': 2,\n",
    "    'sentence': 3,\n",
    "    'url': 4,\n",
    "    'embedded-number': 5,\n",
    "    'list': 6,\n",
    "    'not-generalizable': 7,\n",
    "    'context-specific': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3gN2HUGcVqoH",
    "outputId": "4d1b9a7c-e705-4afd-f17f-50fee930ca53"
   },
   "outputs": [],
   "source": [
    "def ProcessStats(data,y):\n",
    "\n",
    "    data1 = data[['total_vals', 'num_nans', '%_nans', 'num_of_dist_val', '%_dist_val', 'mean', 'std_dev', 'min_val', 'max_val','has_delimiters', 'has_url', 'has_email', 'has_date', 'mean_word_count',\n",
    "       'std_dev_word_count', 'mean_stopword_total', 'stdev_stopword_total',\n",
    "       'mean_char_count', 'stdev_char_count', 'mean_whitespace_count',\n",
    "       'stdev_whitespace_count', 'mean_delim_count', 'stdev_delim_count',\n",
    "       'is_list', 'is_long_sentence']]\n",
    "    data1 = data1.reset_index(drop=True)\n",
    "    data1 = data1.fillna(0)\n",
    "\n",
    "    y.y_act = y.y_act.astype(float)\n",
    "    \n",
    "    return data1\n",
    "\n",
    "vectorizerName = CountVectorizer(ngram_range=(2, 2), analyzer='char')\n",
    "vectorizerSample = CountVectorizer(ngram_range=(2, 2), analyzer='char')\n",
    "\n",
    "def FeatureExtraction(data,data1,flag):\n",
    "\n",
    "    arr = data['Attribute_name'].values\n",
    "    arr = [str(x) for x in arr]\n",
    "    \n",
    "    arr1 = data['sample_1'].values\n",
    "    arr1 = [str(x) for x in arr1]\n",
    "    arr2 = data['sample_2'].values\n",
    "    arr2 = [str(x) for x in arr2]\n",
    "    arr3 = data['sample_3'].values\n",
    "    arr3 = [str(x) for x in arr3]    \n",
    "    print(len(arr1),len(arr2))\n",
    "    if flag:\n",
    "        X = vectorizerName.fit_transform(arr)\n",
    "        X1 = vectorizerSample.fit_transform(arr1)\n",
    "        X2 = vectorizerSample.transform(arr2)   \n",
    "    else:\n",
    "        X = vectorizerName.transform(arr)\n",
    "        X1 = vectorizerSample.transform(arr1)\n",
    "        X2 = vectorizerSample.transform(arr2)        \n",
    "        \n",
    "#     print(f\"> Length of vectorized feature_names: {len(vectorizer.get_feature_names())}\")\n",
    "\n",
    "    attr_df = pd.DataFrame(X.toarray())\n",
    "    sample1_df = pd.DataFrame(X1.toarray())\n",
    "    sample2_df = pd.DataFrame(X2.toarray())\n",
    "    print(len(data1),len(attr_df),len(sample1_df),len(sample2_df))\n",
    "\n",
    "    if useSample1: data2 = sample1_df\n",
    "    if useSample2: data2 = sample2_df    \n",
    "    \n",
    "    data2 = pd.concat([data1, attr_df], axis=1, sort=False)\n",
    "    print(len(data2))\n",
    "    return data2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7936 7936\n",
      "7936 7936 7936 7936\n",
      "7936\n",
      "1985 1985\n",
      "1985 1985 1985 1985\n",
      "1985\n",
      "Training set size: 5158, Simulation set size: 2778, Test set size: 1985\n"
     ]
    }
   ],
   "source": [
    "def get_data(sim_size = 0.9):\n",
    "    \"\"\"\n",
    "    sim_size is the % of training data that goes into simulation set.\n",
    "    \"\"\"\n",
    "    xtrain = pd.read_csv('./data_train.csv')\n",
    "    xtest = pd.read_csv('./data_test.csv')\n",
    "\n",
    "    y_train = xtrain.loc[:,['y_act']]\n",
    "    y_test = xtest.loc[:,['y_act']]\n",
    "    y_train['y_act'] = [dict_label[i] for i in y_train['y_act']]\n",
    "    y_test['y_act'] = [dict_label[i] for i in y_test['y_act']]\n",
    "    \n",
    "    xtrain1 = ProcessStats(xtrain,y_train)\n",
    "    xtest1 = ProcessStats(xtest,y_test)\n",
    "\n",
    "\n",
    "    X_train = FeatureExtraction(xtrain,xtrain1,1)\n",
    "    X_test = FeatureExtraction(xtest,xtest1,0)\n",
    "\n",
    "\n",
    "    X_train_new = X_train.reset_index(drop=True)\n",
    "    y_train_new = y_train.reset_index(drop=True)\n",
    "    X_train_new = X_train_new.values\n",
    "    y_train_new = y_train_new.values\n",
    "    \n",
    "    # setting up data\n",
    "    X_train_AL, X_simulation_AL, y_train_AL, y_simulation_AL = train_test_split(X_train_new, y_train, test_size=sim_size, random_state=4, stratify=y_train, shuffle=True)\n",
    "    # Start AL loops simulated with a part of the test data\n",
    "    X_simulation_df = pd.DataFrame(X_simulation_AL)\n",
    "    y_simulation_df = pd.DataFrame(y_simulation_AL)\n",
    "    # making copies of existing X_test and y_test data\n",
    "    X_test_AL = pd.DataFrame(X_test)\n",
    "    y_test_AL = pd.DataFrame(y_test)\n",
    "    \n",
    "    assert len(X_train_AL) == len(y_train_AL)\n",
    "    assert len(X_simulation_df) == len(y_simulation_df)\n",
    "    assert len(X_test_AL) == len(y_test_AL)\n",
    "    return X_train_AL, y_train_AL, X_simulation_df, y_simulation_df, X_test_AL, y_test_AL\n",
    "\n",
    "X_train, y_train, X_sim, y_sim, X_test, y_test = get_data()\n",
    "print(f\"Training set size: {len(X_train)}, Simulation set size: {len(X_sim)}, Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[n_estimator: 100, max_depth: 100, accuracy: 0.9233261339092873]\n",
      "[n_estimator: 100, max_depth: 200, accuracy: 0.9233261339092873]\n",
      "[n_estimator: 100, max_depth: 300, accuracy: 0.9233261339092873]\n",
      "[n_estimator: 100, max_depth: 400, accuracy: 0.9233261339092873]\n",
      "[n_estimator: 100, max_depth: 500, accuracy: 0.9233261339092873]\n",
      "[n_estimator: 100, max_depth: 600, accuracy: 0.9233261339092873]\n",
      "[n_estimator: 100, max_depth: 700, accuracy: 0.9233261339092873]\n",
      "[n_estimator: 100, max_depth: 800, accuracy: 0.9233261339092873]\n",
      "[n_estimator: 100, max_depth: 900, accuracy: 0.9233261339092873]\n",
      "[n_estimator: 100, max_depth: 1000, accuracy: 0.9233261339092873]\n",
      "[n_estimator: 200, max_depth: 100, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 200, max_depth: 200, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 200, max_depth: 300, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 200, max_depth: 400, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 200, max_depth: 500, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 200, max_depth: 600, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 200, max_depth: 700, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 200, max_depth: 800, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 200, max_depth: 900, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 200, max_depth: 1000, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 300, max_depth: 100, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 300, max_depth: 200, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 300, max_depth: 300, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 300, max_depth: 400, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 300, max_depth: 500, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 300, max_depth: 600, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 300, max_depth: 700, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 300, max_depth: 800, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 300, max_depth: 900, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 300, max_depth: 1000, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 400, max_depth: 100, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 400, max_depth: 200, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 400, max_depth: 300, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 400, max_depth: 400, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 400, max_depth: 500, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 400, max_depth: 600, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 400, max_depth: 700, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 400, max_depth: 800, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 400, max_depth: 900, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 400, max_depth: 1000, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 500, max_depth: 100, accuracy: 0.9265658747300216]\n",
      "[n_estimator: 500, max_depth: 200, accuracy: 0.9265658747300216]\n",
      "[n_estimator: 500, max_depth: 300, accuracy: 0.9265658747300216]\n",
      "[n_estimator: 500, max_depth: 400, accuracy: 0.9265658747300216]\n",
      "[n_estimator: 500, max_depth: 500, accuracy: 0.9265658747300216]\n",
      "[n_estimator: 500, max_depth: 600, accuracy: 0.9265658747300216]\n",
      "[n_estimator: 500, max_depth: 700, accuracy: 0.9265658747300216]\n",
      "[n_estimator: 500, max_depth: 800, accuracy: 0.9265658747300216]\n",
      "[n_estimator: 500, max_depth: 900, accuracy: 0.9265658747300216]\n",
      "[n_estimator: 500, max_depth: 1000, accuracy: 0.9265658747300216]\n",
      "[n_estimator: 600, max_depth: 100, accuracy: 0.925845932325414]\n",
      "[n_estimator: 600, max_depth: 200, accuracy: 0.925845932325414]\n",
      "[n_estimator: 600, max_depth: 300, accuracy: 0.925845932325414]\n",
      "[n_estimator: 600, max_depth: 400, accuracy: 0.925845932325414]\n",
      "[n_estimator: 600, max_depth: 500, accuracy: 0.925845932325414]\n",
      "[n_estimator: 600, max_depth: 600, accuracy: 0.925845932325414]\n",
      "[n_estimator: 600, max_depth: 700, accuracy: 0.925845932325414]\n",
      "[n_estimator: 600, max_depth: 800, accuracy: 0.925845932325414]\n",
      "[n_estimator: 600, max_depth: 900, accuracy: 0.925845932325414]\n",
      "[n_estimator: 600, max_depth: 1000, accuracy: 0.925845932325414]\n",
      "[n_estimator: 700, max_depth: 100, accuracy: 0.925845932325414]\n",
      "[n_estimator: 700, max_depth: 200, accuracy: 0.9262059035277178]\n",
      "[n_estimator: 700, max_depth: 300, accuracy: 0.9262059035277178]\n",
      "[n_estimator: 700, max_depth: 400, accuracy: 0.9262059035277178]\n",
      "[n_estimator: 700, max_depth: 500, accuracy: 0.9262059035277178]\n",
      "[n_estimator: 700, max_depth: 600, accuracy: 0.9262059035277178]\n",
      "[n_estimator: 700, max_depth: 700, accuracy: 0.9262059035277178]\n",
      "[n_estimator: 700, max_depth: 800, accuracy: 0.9262059035277178]\n",
      "[n_estimator: 700, max_depth: 900, accuracy: 0.9262059035277178]\n",
      "[n_estimator: 700, max_depth: 1000, accuracy: 0.9262059035277178]\n",
      "[n_estimator: 800, max_depth: 100, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 800, max_depth: 200, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 800, max_depth: 300, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 800, max_depth: 400, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 800, max_depth: 500, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 800, max_depth: 600, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 800, max_depth: 700, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 800, max_depth: 800, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 800, max_depth: 900, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 800, max_depth: 1000, accuracy: 0.9251259899208063]\n",
      "[n_estimator: 900, max_depth: 100, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 900, max_depth: 200, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 900, max_depth: 300, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 900, max_depth: 400, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 900, max_depth: 500, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 900, max_depth: 600, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 900, max_depth: 700, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 900, max_depth: 800, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 900, max_depth: 900, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 900, max_depth: 1000, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 1000, max_depth: 100, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 1000, max_depth: 200, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 1000, max_depth: 300, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 1000, max_depth: 400, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 1000, max_depth: 500, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 1000, max_depth: 600, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 1000, max_depth: 700, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 1000, max_depth: 800, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 1000, max_depth: 900, accuracy: 0.9254859611231101]\n",
      "[n_estimator: 1000, max_depth: 1000, accuracy: 0.9254859611231101]\n",
      "best model found at ne:500, md:100, at score:0.9265658747300216\n",
      "[BEST OBTAINED RF ESTIMATOR] === [n_estimator: 500, max_depth: 100, accuracy: 0.9265658747300216]\n"
     ]
    }
   ],
   "source": [
    "def grid_parameter_search(X_train, y_train, X_test, y_test, n_estimators_grid, max_depth_grid, query_size):\n",
    "    best_model_score = 0\n",
    "    \n",
    "    preset_batch = partial(uncertainty_batch_sampling, n_instances=query_size)\n",
    "    \n",
    "    for ne in n_estimators_grid:\n",
    "        for md in max_depth_grid:\n",
    "            learner = ActiveLearner(\n",
    "                  estimator=RandomForestClassifier(n_estimators=ne, max_depth=md, random_state=100),\n",
    "                  X_training=X_train, y_training=np.ravel(y_train),\n",
    "                  query_strategy=preset_batch\n",
    "                )\n",
    "            score = learner.score(X_test, y_test)\n",
    "            print(f\"[n_estimator: {ne}, max_depth: {md}, accuracy: {score}]\")\n",
    "            if best_model_score < score:\n",
    "                best_ne = ne\n",
    "                best_md = md\n",
    "                best_model_score = score\n",
    "                bestPerformingModel = learner\n",
    "    print(f\"best model found at ne:{best_ne}, md:{best_md}, at score:{best_model_score}\")\n",
    "    return learner, best_ne, best_md, best_model_score\n",
    "\n",
    "\n",
    "learner, best_ne, best_md, best_model_score = grid_parameter_search(X_train, y_train, X_sim, y_sim, [i*100 for i in range(1,11)],[i*100 for i in range(1,11)], 100)\n",
    "print(f\"[BEST OBTAINED RF ESTIMATOR] === [n_estimator: {best_ne}, max_depth: {best_md}, accuracy: {best_model_score}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PPGdpgmqrEIM"
   },
   "outputs": [],
   "source": [
    "def train_this(n_est, max_dept, query_size, per_iteration_acc_report = True):\n",
    "    \n",
    "    # getting data\n",
    "    X_train, y_train, X_sim, y_sim, X_test, y_test = get_data()\n",
    "    \n",
    "    # init tracker lists\n",
    "    num_queries = []\n",
    "    time_history = []\n",
    "    total_time_history = []\n",
    "    train_acc_history = []\n",
    "    sim_acc_history = []\n",
    "    test_acc_history = []\n",
    "    \n",
    "    # AL cycle\n",
    "    t_start = time.time()\n",
    "    num_of_queries = int(len(X_sim)/query_size) + 1\n",
    "    for i in range(num_of_queries):\n",
    "        print(f\"\\n*********** QUERY {i} ***********remain:{len(X_simulation_df)}\")\n",
    "        t0 = time.time()\n",
    "        \n",
    "        #AL selection\n",
    "        query_idx, query_inst = learner.query(np.array(X_sim))\n",
    "        print(f\"Nodes returned for query in iteration {i}: {query_idx}\")\n",
    "        \n",
    "        #updating learner\n",
    "        learner.teach(X = X_simulation_df.iloc[query_idx], y = y_simulation_df.iloc[query_idx])\n",
    "        feed_to_learner.append(list(query_idx))\n",
    "        \n",
    "        X_simulation_df = X_simulation_df.drop(list(query_idx))\n",
    "        y_simulation_df = y_simulation_df.drop(list(query_idx))\n",
    "\n",
    "        #df version\n",
    "        # delete queries that have been looped back into the model\n",
    "        #X_simulation_df = X_simulation_df.drop(X_simulation_df.index[query_idx])\n",
    "        #y_simulation_df = y_simulation_df.drop(y_simulation_df.index[query_idx])\n",
    "\n",
    "        #np version\n",
    "        # learner.teach(X = pd.Series(X_simulation_np[idx]), y = y_simulation_np[idx])\n",
    "        # X_simulation_np = np.delete(X_simulation_np, query_index, axis=0)\n",
    "        # y_simulation_np = np.delete(y_simulation_np, query_index)\n",
    "        \n",
    "        # recording history\n",
    "        t1 = time.time()\n",
    "        time_history.append(t1 - t0)\n",
    "        total_time_history.append(t1 - t_start)\n",
    "        num_queries.append(len(feed_to_learner))\n",
    "\n",
    "        # Calculate and report our model's accuracy.\n",
    "        if per_iteration_acc_report:\n",
    "            test_acc = learner.score(X_test, y_test)\n",
    "            sim_acc = learner.score(X_sim, y_sim)\n",
    "            train_acc = learner.score(X_train, y_train)\n",
    "                \n",
    "            test_acc_history.append(  test_acc  )\n",
    "            sim_acc_history.append(   sim_acc   )\n",
    "            train_acc_history.append( train_acc )\n",
    "            \n",
    "    \n",
    "            print(f\"\\n[INTERMEDIATE] test_acc:{test_acc}, sim_acc:{sim_acc}, train_acc:{train_acc}\")\n",
    "            \n",
    "        print(f\"\\n[INTERMEDIATE] Time taken for query {i}: {time_history[-1]}\")\n",
    "        print(f\"\\n[INTERMEDIATE] Total time taken {i}: {total_time_history[-1]}\")\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep_percent = 0.2\n",
    "query_sizes = [250,500] #[5, 10, 25, 50, 100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './his/test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_567/3955502060.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#his_file_name = \"ubs_history_{prefix}__ne{best_ne}_md{best_md}_qs{query_size}.csv\"#ubs is uncertainty batch sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mhis_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{his_folder}/{his_file_name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './his/test.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "his_folder = \"./his\"\n",
    "img_folder = \"./img\"\n",
    "feed_his_folder = \"./feed\"\n",
    "\n",
    "test_data_1 = [i for i in range(100)]\n",
    "test_data_2 = [i*2 for i in range(100)]\n",
    "test_data_3 = [i*3 for i in range(100)]\n",
    "test_data_4 = [i*4 for i in range(100)]\n",
    "\n",
    "data = {\"num_queries\": test_data_1, \"time\": test_data_2, \"total_time\":test_data_3, \"test_accuracy\":test_data_4}\n",
    "\n",
    "#his_file_name = \"ubs_history_{prefix}__ne{best_ne}_md{best_md}_qs{query_size}.csv\"#ubs is uncertainty batch sampling\n",
    "his_file_name = \"test.csv\"\n",
    "with open(f\"{his_folder}/{his_file_name}\", \"wb\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(data.keys())\n",
    "    writer.writerows(zip(*d.values()))\n",
    "\n",
    "plt.plot([1] + [i+1 for i in range(1, num_of_queries + 1)], model_accuracies)\n",
    "plt.xlabel(\"Queries\")\n",
    "plt.ylabel(\"Model accuracy on test set\")\n",
    "plt.savefig(f\"./img/{prefix}_ne{best_ne}_md{best_md}_qs{query_size}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project_234_v2.ipynb",
   "provenance": []
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
