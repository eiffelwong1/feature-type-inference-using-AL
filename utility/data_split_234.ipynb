{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_split_234.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNuzrooAYlle0kqOsqu36Ie",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eiffelwong1/feature-type-inference-using-AL/blob/main/colab_notebooks/utility/data_split_234.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS6YTADcMwu5"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import time\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GIrsmp6OcFA",
        "outputId": "734a2bdc-c15d-440a-a231-fd0deb14233c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mk_18idgNhH_",
        "outputId": "c7273a29-2ed4-4d42-8bff-0386eda58642"
      },
      "source": [
        "useStats = 1\n",
        "useAttributeName = 1\n",
        "useSample1 = 0\n",
        "useSample2 = 0\n",
        "\n",
        "dict_label = {\n",
        "    'numeric': 0,\n",
        "    'categorical': 1,\n",
        "    'datetime': 2,\n",
        "    'sentence': 3,\n",
        "    'url': 4,\n",
        "    'embedded-number': 5,\n",
        "    'list': 6,\n",
        "    'not-generalizable': 7,\n",
        "    'context-specific': 8\n",
        "}\n",
        "\n",
        "dict_label_list = [k for k,v in sorted(dict_label.items(), key = lambda x: x[1])]\n",
        "print(dict_label_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['numeric', 'categorical', 'datetime', 'sentence', 'url', 'embedded-number', 'list', 'not-generalizable', 'context-specific']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RsmwB4FM8AG"
      },
      "source": [
        "def ProcessStats(data,y):\n",
        "\n",
        "    data1 = data[['total_vals', 'num_nans', '%_nans', 'num_of_dist_val', '%_dist_val', 'mean', 'std_dev', 'min_val', 'max_val','has_delimiters', 'has_url', 'has_email', 'has_date', 'mean_word_count',\n",
        "       'std_dev_word_count', 'mean_stopword_total', 'stdev_stopword_total',\n",
        "       'mean_char_count', 'stdev_char_count', 'mean_whitespace_count',\n",
        "       'stdev_whitespace_count', 'mean_delim_count', 'stdev_delim_count',\n",
        "       'is_list', 'is_long_sentence']]\n",
        "    data1 = data1.reset_index(drop=True)\n",
        "    data1 = data1.fillna(0)\n",
        "\n",
        "    y.y_act = y.y_act.astype(float)\n",
        "    \n",
        "    return data1\n",
        "\n",
        "vectorizerName = CountVectorizer(ngram_range=(2, 2), analyzer='char')\n",
        "vectorizerSample = CountVectorizer(ngram_range=(2, 2), analyzer='char')\n",
        "\n",
        "def FeatureExtraction(data,data1,flag):\n",
        "\n",
        "    arr = data['Attribute_name'].values\n",
        "    arr = [str(x) for x in arr]\n",
        "    \n",
        "    arr1 = data['sample_1'].values\n",
        "    arr1 = [str(x) for x in arr1]\n",
        "    arr2 = data['sample_2'].values\n",
        "    arr2 = [str(x) for x in arr2]\n",
        "    arr3 = data['sample_3'].values\n",
        "    arr3 = [str(x) for x in arr3]    \n",
        "    #print(len(arr1),len(arr2))\n",
        "    if flag:\n",
        "        X = vectorizerName.fit_transform(arr)\n",
        "        X1 = vectorizerSample.fit_transform(arr1)\n",
        "        X2 = vectorizerSample.transform(arr2)   \n",
        "    else:\n",
        "        X = vectorizerName.transform(arr)\n",
        "        X1 = vectorizerSample.transform(arr1)\n",
        "        X2 = vectorizerSample.transform(arr2)        \n",
        "        \n",
        "#     print(f\"> Length of vectorized feature_names: {len(vectorizer.get_feature_names())}\")\n",
        "\n",
        "    attr_df = pd.DataFrame(X.toarray())\n",
        "    sample1_df = pd.DataFrame(X1.toarray())\n",
        "    sample2_df = pd.DataFrame(X2.toarray())\n",
        "    #print(len(data1),len(attr_df),len(sample1_df),len(sample2_df))\n",
        "\n",
        "    if useSample1: data2 = sample1_df\n",
        "    if useSample2: data2 = sample2_df    \n",
        "    \n",
        "    data2 = pd.concat([data1, attr_df], axis=1, sort=False)\n",
        "    #print(len(data2))\n",
        "    return data2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xut-Ps_DNad4",
        "outputId": "e574f28a-a948-43d9-adba-8c31d6e15bff"
      },
      "source": [
        "def get_data(sim_size = 0.9):\n",
        "    \"\"\"\n",
        "    sim_size is the % of training data that goes into simulation set.\n",
        "    \"\"\"\n",
        "    xtrain = pd.read_csv('/content/drive/My Drive//Project_234/Original/data_train.csv')\n",
        "    xtest = pd.read_csv('/content/drive/My Drive//Project_234/Original/data_test.csv')\n",
        "\n",
        "    y_train = xtrain.loc[:,['y_act']]\n",
        "    y_test = xtest.loc[:,['y_act']]\n",
        "    y_train['y_act'] = [dict_label[i] for i in y_train['y_act']]\n",
        "    y_test['y_act'] = [dict_label[i] for i in y_test['y_act']]\n",
        "    \n",
        "    xtrain1 = ProcessStats(xtrain,y_train)\n",
        "    xtest1 = ProcessStats(xtest,y_test)\n",
        "\n",
        "\n",
        "    X_train = FeatureExtraction(xtrain,xtrain1,1)\n",
        "    X_test = FeatureExtraction(xtest,xtest1,0)\n",
        "\n",
        "\n",
        "    X_train_new = X_train.reset_index(drop=True)\n",
        "    y_train_new = y_train.reset_index(drop=True)\n",
        "    X_train_new = X_train_new.values\n",
        "    y_train_new = y_train_new.values\n",
        "    \n",
        "    # setting up data\n",
        "    X_train_AL, X_simulation_AL, y_train_AL, y_simulation_AL = train_test_split(X_train_new, y_train, test_size=sim_size, random_state=4, stratify=y_train, shuffle=True)\n",
        "    # Start AL loops simulated with a part of the test data\n",
        "    X_simulation_df = pd.DataFrame(X_simulation_AL)\n",
        "    y_simulation_df = pd.DataFrame(y_simulation_AL)\n",
        "    # making copies of existing X_test and y_test data\n",
        "    X_test_AL = pd.DataFrame(X_test)\n",
        "    y_test_AL = pd.DataFrame(y_test)\n",
        "    \n",
        "    assert len(X_train_AL) == len(y_train_AL)\n",
        "    assert len(X_simulation_df) == len(y_simulation_df)\n",
        "    assert len(X_test_AL) == len(y_test_AL)\n",
        "    return X_train_AL, y_train_AL, X_simulation_df, y_simulation_df, X_test_AL, y_test_AL\n",
        "\n",
        "X_train, y_train, X_sim, y_sim, X_test, y_test = get_data()\n",
        "print(f\"Training set size: {len(X_train)}, Simulation set size: {len(X_sim)}, Test set size: {len(X_test)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 793, Simulation set size: 7143, Test set size: 1985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8DxKNdpPEkR",
        "outputId": "95ece257-3857-43a9-c942-9ecfeb841c57"
      },
      "source": [
        "# check distribution counts\n",
        "for v in dict_label.values():\n",
        "  print(f\"Train {v}: {round((y_train['y_act'] == v).sum()/ len(y_train), 3) * 100}%\")\n",
        "  print(f\"Test {v}: {round((y_test['y_act'] == v).sum()/ len(y_test), 3) * 100}%\")\n",
        "  print(f\"Sim {v}: {round((y_sim['y_act'] == v).sum()/ len(y_sim), 3) * 100}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 0: 36.6%\n",
            "Test 0: 35.6%\n",
            "Sim 0: 36.6%\n",
            "Train 1: 23.3%\n",
            "Test 1: 23.0%\n",
            "Sim 1: 23.400000000000002%\n",
            "Train 2: 6.800000000000001%\n",
            "Test 2: 7.1%\n",
            "Sim 2: 6.800000000000001%\n",
            "Train 3: 3.6999999999999997%\n",
            "Test 3: 4.6%\n",
            "Sim 3: 3.6999999999999997%\n",
            "Train 4: 1.5%\n",
            "Test 4: 1.6%\n",
            "Sim 4: 1.4000000000000001%\n",
            "Train 5: 5.8999999999999995%\n",
            "Test 5: 5.0%\n",
            "Sim 5: 5.8999999999999995%\n",
            "Train 6: 2.4%\n",
            "Test 6: 2.9000000000000004%\n",
            "Sim 6: 2.4%\n",
            "Train 7: 11.0%\n",
            "Test 7: 10.8%\n",
            "Sim 7: 11.0%\n",
            "Train 8: 8.799999999999999%\n",
            "Test 8: 9.3%\n",
            "Sim 8: 8.9%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyS7c2o7RetH"
      },
      "source": [
        "pd.DataFrame(X_train).to_csv(\"/content/drive/My Drive/Project_234/AL_Split/split_train_X.csv\")\n",
        "pd.DataFrame(y_train).to_csv(\"/content/drive/My Drive/Project_234/AL_Split/split_train_y.csv\")\n",
        "X_sim.to_csv(\"/content/drive/My Drive/Project_234/AL_Split/split_sim_X.csv\")\n",
        "y_sim.to_csv(\"/content/drive/My Drive/Project_234/AL_Split/split_sim_y.csv\") \n",
        "X_test.to_csv(\"/content/drive/My Drive/Project_234/AL_Split/split_test_X.csv\") \n",
        "y_test.to_csv(\"/content/drive/My Drive/Project_234/AL_Split/split_test_y.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}